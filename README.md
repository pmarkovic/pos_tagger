# A neural POS tagger
Implementation of a neural model for the Part-Of-Speech tagging task for English language.

## Table of Contents
* [General Information](#General-info)
* [Technologies](#Technologies)
* [Setup](#Setup)

## General Information
Part-Of-Speech tagging is a NLP task for labeling words in a text with proper Part-Of-Speech tags. This task is useful for analyzing linguistic properties of a text, but it can be used for improving results on other tasks (e.g. sentiment analysis) as well. Example (from the Speech and Language Processing book, ch.8):
> There/PRO/EX are/VERB/VBP 70/NUM/CD children/NOUN/NNSthere/ADV/RB ./PUNC/

In this project, a neural model is trained on sample.conll data to perform the task.

## Technologies
For the project development, following technologies are used:
- Python: 3.8
- Numpy
- PyTorch
- HuggingFace transformers

## Setup

### Project Directory Structure (TODO: update in the end)
```
.
├── data_preprocessing
│   ├── data_preprocess.py
│   └── data_preprocess.sh
├── embeddings
│   ├── ptag2embedding.pkl
│   └── train_examples
│       ├── examples_10.pkl
│       ├── examples_11.pkl
│       ├── examples_12.pkl
│       ├── examples_1.pkl
│       ├── examples_2.pkl
│       ├── examples_3.pkl
│       ├── examples_4.pkl
│       ├── examples_5.pkl
│       ├── examples_6.pkl
│       ├── examples_7.pkl
│       ├── examples_8.pkl
│       └── examples_9.pkl
├── gen_embeddings.py
├── model.py
├── pavle_rricha.yml
├── README.md
├── test_data
│   ├── ontonotes-4.0.info
│   ├── ontonotes-4.0.tsv
│   └── test_set.tsv
├── train.py
└── util.py


```

### Create environment
- Step 0) Clone the repo: `git clone https://github.com/pmarkovic/pos_tagger.git`
- Step 1) Create conda environment: `conda env create -f pavle_rricha.yml`

### Data Preprocessing (TODO: update)
- Step 0) Enter into the directory of the project from the terminal: `cd pos_tagger`.   
- Step 1) Generate `sample.conll` file from the given `.conll` files using the command `cat ./data/given_files/*.gold_conll > data/sample.conll` from the terminal.  
- Step 2) For preprocessing, simply run the command `python3 data_preprocessing/data_preprocess.py --input_file=data/sample.conll --output_dir=data/` or `data_preprocessing/data_preprocess.sh` file. 

### Generate training examples
- Step 0) To generate training examples run (specify paths if needed): `python3 gen_embeddings.py --gen_embed`
- Step 1) Remove ./temp_data (or other specified dir) with intermediate results: `rm -r ./temp_data`
```
usage: gen_embeddings.py [-h] [--train TRAIN] [--test TEST] [--temp_dir TEMP_DIR] [--examples_dir EXAMPLES_DIR] [--ptag_emb PTAG_EMB] [--bert_model BERT_MODEL] [--gen_embed] [--batch_size BATCH_SIZE]

optional arguments:
  -h, --help            show this help message and exit
  --train TRAIN         use this option to provide the path to English POS-tagged data
  --test TEST           use this option to provide the path to combined srb-de POS-tagged data
  --temp_dir TEMP_DIR   directory where to save intermediate train examples (default=./temp_data).
  --examples_dir EXAMPLES_DIR
                        directory where to save final train examples (default=./embeddings/train_examples).
  --ptag_emb PTAG_EMB   use this option to provide the path to pickled POS-tag embeddings (default: "./embeddings")
  --bert_model BERT_MODEL
                        bert model to use for encoding text (default=bert-base-multilingual-cased).
  --gen_embed           flag to indicate if embeddings should be generated (default=False).
  --batch_size BATCH_SIZE
                        batches of sentences will be created with the given size for generating embeddings (default=50)

```

### Train 
-  Run `python3 train.py`
```
usage: train.py [-h] [--model_path MODEL_PATH] [--bert_model BERT_MODEL] [--mdim MDIM] [--bdim BDIM] [--seed SEED] [--k K] [--n N] [--ep EP] [--lr LR] [--train TRAIN] [--test TEST] [--ptag_emb PTAG_EMB]

optional arguments:
  -h, --help            show this help message and exit
  --model_path MODEL_PATH
                        path where to save the trained model (default=models/model.pt).
  --bert_model BERT_MODEL
                        bert model to use for encoding text (default=bert-base-multilingual-cased).
  --mdim MDIM           dimension of embeddings in a metric space (default=512).
  --bdim BDIM           dimension of bert model embeddings (default=768).
  --seed SEED           seed for reproducibility purpose (default=777).
  --k K                 number of classes/tags per episode (default=10).
  --n N                 number of examples/shots per class per episode (default=5).
  --ep EP               number of episodes per training (default=100).
  --lr LR               learning rate (default=1e-3).
  --train TRAIN         use this option to provide the path to pickled training examples that were generated by gen_embeddings.pydefault: ./embeddings/train_examples
  --test TEST           use this option to provide the path to the combined srb-de POS-tagged data
  --ptag_emb PTAG_EMB   use this option to provide the path to pickled POS-tag embeddings (default: "./embeddings/ptag2embedding.pkl")
```

### Test
- Run `python3 test.py`
```
usage: test.py [-h] [--model MODEL] [--test TEST] [--bert_model BERT_MODEL]
               [--ptag_emb PTAG_EMB] [--batch_size BATCH_SIZE]

optional arguments:
  -h, --help            show this help message and exit
  --model MODEL         path to the trained model (default=./models/model.pt).
  --test TEST           path to the test set (default=./test_data/de-
                        test.tsv).
  --bert_model BERT_MODEL
                        bert model to use for encoding text (default=bert-
                        base-multilingual-cased).
  --ptag_emb PTAG_EMB   use this option to provide the path to pickled POS-tag
                        embeddings (default: "./embeddings")
  --batch_size BATCH_SIZE
                        batches of sentences will be created with the given
                        size for generating embeddings (default=50)
```